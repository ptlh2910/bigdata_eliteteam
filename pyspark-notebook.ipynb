{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PySpark**: The Apache Spark Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook shows how to connect Jupyter notebooks to a Spark cluster to process data using Spark Python API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Spark Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Connection\n",
    "\n",
    "To connect to the Spark cluster, create a SparkSession object with the following params:\n",
    "\n",
    "+ **appName:** application name displayed at the [Spark Master Web UI](http://localhost:8080/);\n",
    "+ **master:** Spark Master URL, same used by Spark Workers;\n",
    "+ **spark.executor.memory:** must be less than or equals to docker compose SPARK_WORKER_MEMORY config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/05 21:22:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5c255da36bca:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0e4afda820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"512m\").\\\n",
    "        config(\"spark.driver.memory\",\"16G\").\\\n",
    "        config(\"spark.driver.maxResultSize\", \"0\").\\\n",
    "        getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sparknlp in /usr/local/lib/python3.9/dist-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from sparknlp) (1.22.0)\n",
      "Requirement already satisfied: spark-nlp in /usr/local/lib/python3.9/dist-packages (from sparknlp) (3.4.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sparknlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More confs for SparkSession object in standalone mode can be added using the **config** method. Checkout the API docs [here](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SparkSession)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Introduction\n",
    "\n",
    "We will be using Spark Python API to read, process and write data. Checkout the API docs [here](https://spark.apache.org/docs/latest/api/python/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Read\n",
    "\n",
    "Let's read some UK's macroeconomic data ([source](https://www.kaggle.com/bank-of-england/a-millennium-of-macroeconomic-data)) from the cluster's simulated **Hadoop distributed file system (HDFS)** into a Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dataset = spark.read \\\n",
    "#       .option(\"header\", True) \\\n",
    "#       .json(\"data/Prime_Pantry.json\")\n",
    "dataset = spark.read \\\n",
    "      .option(\"header\", True) \\\n",
    "      .option(\"inferSchema\", True) \\\n",
    "      .option('quote', '\"') \\\n",
    "      .option('escape', '\"') \\\n",
    "      .csv(\"hdfs://namenode:9000/data/preprocessed_data.csv\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then display some dataframe metadata, such as the number of rows and cols and its schema (cols name and type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand \n",
    "dataset = dataset.orderBy(rand()) # Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+\n",
      "| _c0|              inputs|           outputs|\n",
      "+----+--------------------+------------------+\n",
      "|  23|cổ_điển nổi_tiếng...|         am-nhac-1|\n",
      "|3872|ươm mầm trí_tuệ t...|my-thuat-kien-truc|\n",
      "|4451|phương_pháp nuôi ...|   nong-lam-nghiep|\n",
      "|6445|bóng_đá 12 vì tin...|          the-thao|\n",
      "|6862|phim đời vượt lên...|van-hoa-nghe-thuat|\n",
      "+----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# regular expression tokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"inputs\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# stop words\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "# bag of words count\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------------------+--------------------+--------------------+--------------------+-----+\n",
      "| _c0|              inputs|           outputs|               words|            filtered|            features|label|\n",
      "+----+--------------------+------------------+--------------------+--------------------+--------------------+-----+\n",
      "|  23|cổ_điển nổi_tiếng...|         am-nhac-1|[c, _, i, n, n, i...|[_, i, n, n, i_ti...|(3929,[0,1,2,3,4,...| 11.0|\n",
      "|3872|ươm mầm trí_tuệ t...|my-thuat-kien-truc|[m, m, m, tr, _tu...|[m, m, m, tr, _tu...|(3929,[0,1,3,4,5,...|  8.0|\n",
      "|4451|phương_pháp nuôi ...|   nong-lam-nghiep|[ph, ng_ph, p, nu...|[ph, ng_ph, p, nu...|(3929,[0,1,2,3,4,...|  7.0|\n",
      "|6445|bóng_đá 12 vì tin...|          the-thao|[b, ng_, 12, v, t...|[b, ng_, 12, v, t...|(3929,[0,1,2,3,4,...|  9.0|\n",
      "|6862|phim đời vượt lên...|van-hoa-nghe-thuat|[phim, i, v, t, l...|[phim, i, v, l, n...|(3929,[0,1,2,3,4,...| 12.0|\n",
      "+----+--------------------+------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "label_stringIdx = StringIndexer(inputCol = \"outputs\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx])\n",
    "# Fit the pipeline to training documents.\n",
    "pipelineFit = pipeline.fit(dataset)\n",
    "dataset = pipelineFit.transform(dataset)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| _c0|              inputs|             outputs|               words|            filtered|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|  23|cổ_điển nổi_tiếng...|           am-nhac-1|[c, _, i, n, n, i...|[_, i, n, n, i_ti...|(3929,[0,1,2,3,4,...| 11.0|\n",
      "| 571|lão tử đạo_đức ki...| chinh-tri-triet-hoc|[l, o, t, o_, c, ...|[l, o, o_, kinh, ...|(3929,[0,1,2,3,4,...|  5.0|\n",
      "| 770|nha trang_điểm hẹ...|           du-lich-1|[nha, trang_, i, ...|[nha, trang_, i, ...|(3929,[0,1,2,3,4,...| 13.0|\n",
      "| 936|hỗn_độn hài_hòa t...|     khoa-hoc-co-ban|[h, n_, n, h, i_h...|[h, n_, n, h, i_h...|(3929,[0,1,2,3,4,...| 10.0|\n",
      "| 979|sổ_tay kiến_thức_...|     khoa-hoc-co-ban|[s, _tay, ki, n_t...|[s, _tay, ki, n_t...|(3929,[0,1,2,3,6,...| 10.0|\n",
      "|1419|thiết_kế vi_mạch ...|   khoa-hoc-ky-thuat|[thi, t_k, vi_m, ...|[thi, t_k, vi_m, ...|(3929,[0,1,3,4,7,...|  6.0|\n",
      "|1475|công_nghệ tạo_hìn...|   khoa-hoc-ky-thuat|[c, ng_ngh, t, o_...|[ng_ngh, o_h, nh,...|(3929,[0,1,2,3,4,...|  6.0|\n",
      "|1649|about muốn tái_bả...|khoa-hoc-tu-nhien...|[about, mu, n, t,...|[about, mu, n, i_...|(3929,[0,1,2,3,4,...|  3.0|\n",
      "|1730|bên quả trứng gì ...|khoa-hoc-tu-nhien...|[b, n, qu, tr, ng...|[b, n, qu, tr, ng...|(3929,[0,1,2,3,4,...|  3.0|\n",
      "|2240|hãy trả_lời em tạ...|khoa-hoc-tu-nhien...|[h, y, tr, _l, i,...|[h, y, tr, _l, i,...|(3929,[0,1,2,3,4,...|  3.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 98:==================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| _c0|              inputs|             outputs|               words|            filtered|            features|label|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| 808|500 danh_lam 500 ...|           du-lich-1|[500, danh_lam, 5...|[500, danh_lam, 5...|(3929,[0,1,2,3,4,...| 13.0|\n",
      "|1951|trắc_nghiệm thông...|khoa-hoc-tu-nhien...|[tr, c_nghi, m, t...|[tr, c_nghi, m, t...|(3929,[0,1,2,3,4,...|  3.0|\n",
      "|2545|hoàng lê nhất_thố...|      lich-su-dia-ly|[ho, ng, l, nh, t...|[ho, ng, l, nh, t...|(3929,[0,1,2,3,4,...|  0.0|\n",
      "|3961|1000 gương_mặt th...|  my-thuat-kien-truc|[1000, g, ng_m, t...|[1000, g, ng_m, t...|(3929,[0,1,2,3,4,...|  8.0|\n",
      "|4746|luật thuế thu_nhậ...|         phap-luat-1|[lu, t, thu, thu_...|[lu, thu, thu_nh,...|(3929,[0,1,2,3,4,...|  4.0|\n",
      "|5361|thiền_sư em bé 5 ...|       sach-ton-giao|[thi, n_s, em, b,...|[thi, n_s, em, b,...|(3929,[0,1,2,3,4,...|  2.0|\n",
      "|6791|nổi_tiếng khuôn_m...|  van-hoa-nghe-thuat|[n, i_ti, ng, khu...|[n, i_ti, ng, khu...|(3929,[0,1,2,3,4,...| 12.0|\n",
      "|6862|phim đời vượt lên...|  van-hoa-nghe-thuat|[phim, i, v, t, l...|[phim, i, v, l, n...|(3929,[0,1,2,3,4,...| 12.0|\n",
      "|7080|lối lành_mạnh phò...|               y-hoc|[l, i, l, nh_m, n...|[l, i, l, nh_m, n...|(3929,[0,1,2,3,4,...|  1.0|\n",
      "|7670|y tướng học truyề...|               y-hoc|[y, t, ng, h, c, ...|[y, ng, h, truy, ...|(3929,[0,1,2,3,4,...|  1.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 100)\n",
    "trainingData.show(10)\n",
    "testData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/05 21:41:32 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/01/05 21:41:32 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/01/05 21:43:55 WARN TaskSetManager: Lost task 73.0 in stage 343.0 (TID 18205, 172.31.0.10, executor 0): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 12042556 bytes of memory, got 10678061\n",
      "\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:118)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:381)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:405)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:99)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "22/01/05 21:44:49 WARN TaskSetManager: Lost task 73.0 in stage 423.0 (TID 23951, 172.31.0.11, executor 1): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 12042556 bytes of memory, got 10678061\n",
      "\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:118)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:381)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:405)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:138)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n",
      "\tat org.apache.spark.sql.execution.columnar.CachedRDDBuilder$$anon$1.hasNext(InMemoryRelation.scala:132)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:221)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:299)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1371)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1298)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1362)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1186)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:360)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:311)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.$anonfun$compute$1(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.sql.internal.SQLConf$.withExistingConf(SQLConf.scala:99)\n",
      "\tat org.apache.spark.sql.execution.SQLExecutionRDD.compute(SQLExecutionRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:313)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:444)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:447)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.1]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cvModel = cv.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will get UK's population and unemployment rate thoughtout the years. Let's start by selecting the relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+------------------------+------------------------------+-----+----------+\n",
      "|                        inputs|                 outputs|                   probability|label|prediction|\n",
      "+------------------------------+------------------------+------------------------------+-----+----------+\n",
      "|hoàng lê nhất_thống chí diễ...|          lich-su-dia-ly|[0.4643103572852561,0.09053...|  0.0|       0.0|\n",
      "|1000 gương_mặt thượng_đế ng...|      my-thuat-kien-truc|[0.15277156658172505,0.0922...|  8.0|       0.0|\n",
      "|nhà_tù côn_đảo 1862 1975 nh...|          lich-su-dia-ly|[0.5679016755962756,0.03807...|  0.0|       0.0|\n",
      "|dân_ca làn_điệu dân_ca phổ_...|               am-nhac-1|[0.21366076468261685,0.0953...| 11.0|       0.0|\n",
      "|mandala thiên_nhiên tô_màu ...|      my-thuat-kien-truc|[0.147508900349656,0.113460...|  8.0|       0.0|\n",
      "|tìm_hiểu kiến_trúc xây_dựng...|      my-thuat-kien-truc|[0.1489358689412086,0.11637...|  8.0|       0.0|\n",
      "|vương_quốc đàng ngoài tái_b...|          lich-su-dia-ly|[0.49722480367858013,0.0403...|  0.0|       0.0|\n",
      "|mỹ_thuật căn_bản nâng cao v...|      my-thuat-kien-truc|[0.1378605335839985,0.10880...|  8.0|       0.0|\n",
      "|kỹ_thuật trồng một_số cây t...|         nong-lam-nghiep|[0.14308601920848357,0.1262...|  7.0|       0.0|\n",
      "|tủ_sách kỹ_năng hoạt_động t...|khoa-hoc-tu-nhien-xa-hoi|[0.14103364129058957,0.1096...|  3.0|       0.0|\n",
      "|ca_tụng thân_xác ca_tụng th...|khoa-hoc-tu-nhien-xa-hoi|[0.15188152141425185,0.1227...|  3.0|       0.0|\n",
      "|miến điện đất_nước hình ngọ...|               du-lich-1|[0.3206342367375058,0.06989...| 13.0|       0.0|\n",
      "|combo tài_liệu mật 3 1 tài_...|          lich-su-dia-ly|[0.754116640315349,0.023695...|  0.0|       0.0|\n",
      "|100 ván cờ xuất_sắc vua cờ ...|                the-thao|[0.27519363345378905,0.0883...|  9.0|       0.0|\n",
      "|tuyển_tập 100 bài hát sinh_...|khoa-hoc-tu-nhien-xa-hoi|[0.20447439472771084,0.1362...|  3.0|       0.0|\n",
      "|di_sản văn_hóa phương đây ý...|          lich-su-dia-ly|[0.17008040727186782,0.1075...|  0.0|       0.0|\n",
      "|chân_dung nữ anh_hùng thời_...|          lich-su-dia-ly|[0.469901629114331,0.055515...|  0.0|       0.0|\n",
      "|230 loài gỗ thường gặp sản_...|         nong-lam-nghiep|[0.13225674693040415,0.1115...|  7.0|       0.0|\n",
      "|đổi_mới nâng cao phương_hướ...|             phap-luat-1|[0.14861075833824328,0.1090...|  4.0|       0.0|\n",
      "|kỹ_thuật móc len sợi căn_bả...|           sach-hoc-nghe|[0.13029861721429636,0.0928...| 15.0|       0.0|\n",
      "+------------------------------+------------------------+------------------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = cvModel.transform(testData)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"inputs\",\"outputs\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .show(n = 20, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6420433316892963"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully selected the desired columns but two problems were found:\n",
    "+ The first line contains no data but the unit of measurement of each column;\n",
    "+ There are many years with missing population and unemployment data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then remove the first line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "     |████████████████████████████████| 26.4 MB 874 kB/s            \n",
      "\u001b[?25hCollecting joblib>=0.11\n",
      "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "     |████████████████████████████████| 306 kB 2.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->sklearn) (1.22.0)\n",
      "Collecting scipy>=1.1.0\n",
      "  Downloading scipy-1.7.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.8 MB)\n",
      "     |████████████████████████████████| 39.8 MB 676 kB/s            \n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=6496816f2187fd14411ca5d8ef75238edc3e9e2003517d3d01071b41a4d0d19c\n",
      "  Stored in directory: /root/.cache/pip/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built sklearn\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn, sklearn\n",
      "Successfully installed joblib-1.1.0 scikit-learn-1.0.2 scipy-1.7.3 sklearn-0.0 threadpoolctl-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now, let's drop the dataframe rows with missing data and refactor its columns names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We are going to use sklearn to evalute the results on test dataset\n",
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(predictions['prediction'], predictions['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we persist the unemployment data into the cluster's simulated **HDFS**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
